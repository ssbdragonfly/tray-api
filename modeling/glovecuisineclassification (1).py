# -*- coding: utf-8 -*-
"""GloveCuisineClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VXp8r4RvdAee99BrO_AvmgmykeuGjWe5
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Conv1D, MaxPooling1D
from tensorflow.keras.optimizers import Adam

train = pd.read_json('train.json')
train['ingredients_str'] = train['ingredients'].apply(lambda x: ' '.join(x))

le = LabelEncoder()
train['cuisine'] = le.fit_transform(train['cuisine'])

X_train, X_test, y_train, y_test = train_test_split(train['ingredients_str'], train['cuisine'], test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_len = 100
X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

embeddings_index = {}
with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedding_dim = 100
word_index = tokenizer.word_index
num_words = min(20000, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, embedding_dim))
for word, i in word_index.items():
    if i < 20000:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

model = Sequential([
    Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    MaxPooling1D(pool_size=2),
    Bidirectional(LSTM(128, return_sequences=True)),
    BatchNormalization(),
    Dropout(0.5),
    Bidirectional(LSTM(64)),
    BatchNormalization(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(le.classes_), activation='softmax')
])

optimizer = Adam(learning_rate=0.0001)
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
history = model.fit(X_train_padded, y_train, epochs=50, validation_data=(X_test_padded, y_test), batch_size=64)

model.save('improvedv3_cuisine_classification_model.h5')

loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

